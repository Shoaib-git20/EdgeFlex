{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cea9395-53e3-45da-b082-3b23b9c83afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $PWD\n",
    "!which python\n",
    "\n",
    "!echo $CUDA_HOME\n",
    "!echo $CUDNN_LIB_DIR\n",
    "!echo $CUDNN_INCLUDE_DIR\n",
    "!echo $LD_LIBRARY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75162a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.quantization\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "import torch.nn.utils.prune as prune\n",
    "from torch.cuda.amp import autocast\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494dec42-c6b3-476d-a64e-96dd17a363d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())  # Should return True if CUDA is available\n",
    "print(torch.cuda.current_device())  # Should return the device index\n",
    "print(torch.cuda.get_device_name(0))  # Should return the name of the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951bfd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration\n",
    "model_name = \"google/vit-huge-patch14-224-in21k\"\n",
    "PRUNE_PERCENTILE = 30\n",
    "BATCH_SIZE = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197bed0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load Models and Feature Extractor\n",
    "original_fp32_model = ViTForImageClassification.from_pretrained(model_name, num_labels=10)\n",
    "pruned_fp32_model = ViTForImageClassification.from_pretrained(model_name, num_labels=10)\n",
    "pruned_awq_model = ViTForImageClassification.from_pretrained(model_name, num_labels=10)\n",
    "pruned_fp16_model = ViTForImageClassification.from_pretrained(model_name, num_labels=10)\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4b9fcd-f55a-4027-8fc9-5c04c475142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load and Prepare Calibration Data\n",
    "# --- Load CIFAR-10 Datasets ---\n",
    "\n",
    "# Load CIFAR-10 without transform because feature extractor handles it\n",
    "cifar10_dataset = CIFAR10(root='./data', train=False, download=True, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102ad672-1959-4344-a4d6-bd28dadf2020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full test set\n",
    "full_test_images, full_test_labels = zip(*[cifar10_dataset[i] for i in range(len(cifar10_dataset))])\n",
    "full_test_labels = torch.tensor(full_test_labels)\n",
    "\n",
    "# Display a few sample images\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.imshow(full_test_images[i])\n",
    "    plt.title(f\"Label: {full_test_labels[i]}\")\n",
    "    plt.axis('off')\n",
    "plt.suptitle(\"full Sample Images\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3b3aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration subset (32 samples)\n",
    "calibration_indices = list(range(32))\n",
    "calibration_subset = Subset(cifar10_dataset, calibration_indices)\n",
    "calibration_images, calibration_labels = zip(*[calibration_subset[i] for i in range(len(calibration_subset))])\n",
    "calibration_labels = torch.tensor(calibration_labels)\n",
    "\n",
    "# Display a few sample images\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.imshow(calibration_images[i])\n",
    "    plt.title(f\"Label: {calibration_labels[i]}\")\n",
    "    plt.axis('off')\n",
    "plt.suptitle(\"Calibration Sample Images\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183e2f00-f00a-4704-a04f-c3b23f824592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract inputs using feature extractor\n",
    "calib_inputs = feature_extractor(images=calibration_images, return_tensors=\"pt\")\n",
    "test_inputs = feature_extractor(images=full_test_images, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f846050",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --- Register Hooks on Linear Layers for Activation Collection ---\n",
    "\n",
    "# Use original_fp32_model for profiling\n",
    "\n",
    "activation_stats = {}\n",
    "memory_stats = {}\n",
    "\n",
    "def activation_and_memory_hook(module, input, output):\n",
    "    with torch.no_grad():\n",
    "        output = output[0]\n",
    "        if output.dim() == 2:\n",
    "            avg_activation = output.abs().mean(dim=0)\n",
    "        elif output.dim() == 3:\n",
    "            avg_activation = output.abs().mean(dim=[0, 1])\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected output dimensions: {output.dim()}\")\n",
    "        mem_bytes = output.numel() * output.element_size()\n",
    "    \n",
    "    module_key = f\"{str(module)}_{id(module)}\"\n",
    "    activation_stats[module_key] = avg_activation.cpu()\n",
    "    memory_stats[module_key] = mem_bytes\n",
    "\n",
    "hooks = []\n",
    "for i, encoder_layer in enumerate(original_fp32_model.vit.encoder.layer):\n",
    "    for sub_name, sub_module in encoder_layer.named_modules():\n",
    "        if isinstance(sub_module, torch.nn.Linear) and (\n",
    "            \"intermediate.dense\" in sub_name or \"output.dense\" in sub_name\n",
    "        ):\n",
    "            hook = sub_module.register_forward_hook(activation_and_memory_hook)\n",
    "            hooks.append(hook)\n",
    "            print(f\"Registered hook on: {sub_name}, id: {id(sub_module)}\")\n",
    "\n",
    "# Run forward passes to collect activations\n",
    "original_fp32_model.to(device)\n",
    "batch_size = 4\n",
    "num_samples = calib_inputs['pixel_values'].size(0)\n",
    "\n",
    "for i in range(0, num_samples, batch_size):\n",
    "    batch_inputs = {k: v[i:i+batch_size].to(device) for k, v in calib_inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        _ = original_fp32_model(**batch_inputs)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# --- STEP 5: Clean Up Hooks ---\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nHooks removed and activations stored.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476ba9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Pruning\n",
    "def prune_model(model, activation_stats, percentile=10):\n",
    "    for encoder_layer in model.vit.encoder.layer:\n",
    "        for name, module in encoder_layer.named_modules():\n",
    "            if isinstance(module, torch.nn.Linear) and (\"intermediate.dense\" in name or \"output.dense\" in name):\n",
    "                module_key = f\"{str(module)}_{id(module)}\"\n",
    "                if module_key in activation_stats:\n",
    "                    act_stat = activation_stats[module_key].numpy()\n",
    "                    threshold = np.percentile(act_stat, percentile)\n",
    "                    channel_mask = (act_stat >= threshold).astype(np.float32)\n",
    "                    channel_mask_tensor = torch.tensor(channel_mask, dtype=torch.float32, device=module.weight.device)\n",
    "                    if module.weight.dim() >= 2:\n",
    "                        mask = channel_mask_tensor.unsqueeze(-1).expand_as(module.weight)\n",
    "                    else:\n",
    "                        mask = channel_mask_tensor\n",
    "                    prune.custom_from_mask(module, name=\"weight\", mask=mask)\n",
    "\n",
    "prune_model(pruned_fp32_model, activation_stats, percentile=PRUNE_PERCENTILE)\n",
    "prune_model(pruned_awq_model, activation_stats, percentile=PRUNE_PERCENTILE)\n",
    "prune_model(pruned_fp16_model, activation_stats, percentile=PRUNE_PERCENTILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc687ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Quantization (AWQ)\n",
    "def awq_quantize_model(model, activation_stats, bitwidth=8):\n",
    "    for name, module in model.named_modules():\n",
    "        module_key = f\"{str(module)}_{id(module)}\"\n",
    "        if isinstance(module, torch.nn.Linear) and module_key in activation_stats:\n",
    "            act_stat = activation_stats[module_key].numpy()\n",
    "            weight = module.weight.data.cpu().numpy()\n",
    "            out_channels = weight.shape[0]\n",
    "            scales = np.zeros(out_channels)\n",
    "            for i in range(out_channels):\n",
    "                max_w = np.max(np.abs(weight[i]))\n",
    "                max_a = act_stat[i]\n",
    "                scale = max(max_w * max_a, 1e-5)\n",
    "                scales[i] = scale / (2 ** (bitwidth - 1) - 1)\n",
    "            weight_q = np.round(weight / scales[:, None]).clip(\n",
    "                -(2 ** (bitwidth - 1)),\n",
    "                2 ** (bitwidth - 1) - 1\n",
    "            ).astype(np.int8)\n",
    "            # Optional: Dequantize if needed\n",
    "            # weight_deq = (weight_q * scales[:, None]).astype(np.float32)\n",
    "            # module.weight.data = torch.tensor(weight_deq, device=module.weight.device)\n",
    "\n",
    "awq_quantize_model(pruned_awq_model, activation_stats, bitwidth=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c43714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Convert to FP16\n",
    "pruned_fp16_model = pruned_fp16_model.half()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fb8f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Evaluation Function\n",
    "def evaluate_model(model, variant_name=\"\", use_amp=False, inputs=test_inputs, labels=full_test_labels):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    latencies = []\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    model.to(device)\n",
    "    if use_amp:\n",
    "        inputs = {k: v.half().to(device) for k, v in inputs.items()}\n",
    "    start_total = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, labels.size(0), BATCH_SIZE):\n",
    "            batch_inputs = {k: v[i:i+BATCH_SIZE].to(device) for k, v in inputs.items()}\n",
    "            batch_labels = labels[i:i+BATCH_SIZE].to(device)\n",
    "            torch.cuda.synchronize()\n",
    "            start_batch = time.time()\n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    outputs = model(**batch_inputs)\n",
    "            else:\n",
    "                outputs = model(**batch_inputs)\n",
    "            torch.cuda.synchronize()\n",
    "            latencies.append(time.time() - start_batch)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            correct += (preds == batch_labels).sum().item()\n",
    "            total += batch_labels.size(0)\n",
    "    total_time = time.time() - start_total\n",
    "    accuracy = 100.0 * correct / total\n",
    "    avg_latency = sum(latencies) / len(latencies)\n",
    "    peak_memory = torch.cuda.max_memory_allocated(device) / (1024 ** 2)\n",
    "    return {\n",
    "        \"Variant\": variant_name,\n",
    "        \"Accuracy (%)\": accuracy,\n",
    "        \"Avg Batch Latency (s)\": avg_latency,\n",
    "        \"Peak Memory (MB)\": peak_memory,\n",
    "        \"Total Inference Time (s)\": total_time\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0ccbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Evaluate Variants\n",
    "variants = {\n",
    "    \"Original FP32\": {\"model\": original_fp32_model, \"use_amp\": False},\n",
    "    \"Pruned FP32\": {\"model\": pruned_fp32_model, \"use_amp\": False},\n",
    "    \"Pruned + Quantized\": {\"model\": pruned_awq_model, \"use_amp\": False},\n",
    "    \"Pruned + FP16\": {\"model\": pruned_fp16_model, \"use_amp\": True},\n",
    "}\n",
    "\n",
    "benchmark_results = []\n",
    "for name, config in variants.items():\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    result = evaluate_model(config[\"model\"], variant_name=name, use_amp=config[\"use_amp\"])\n",
    "    benchmark_results.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a6ed08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Display Results\n",
    "df = pd.DataFrame(benchmark_results)\n",
    "print(\"\\nBenchmark Results:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3089a22c-db7a-4993-af23-a1b24b350ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
